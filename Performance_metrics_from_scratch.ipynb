{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Computing performance metrics from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [],
   "source": [
    "dt_5_a = pd.read_csv('5_a.csv')\n",
    "prob = np.asarray(dt_5_a['proba'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "source": [
    "#### Deriving class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [],
   "source": [
    "y_pred = np.asarray(list(map(lambda x: 0 if x < 0.5 else 1, prob)))\n",
    "y_obs = np.asarray(dt_5_a['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "source": [
    "#### Calculating performance metrics\n",
    "##### Confusion Matrix, F1 Score and Accuaracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "             Actual:0  Actual:1\n",
      "Predicted:0         0         0\n",
      "Predicted:1       100     10000\n",
      "\n",
      "F1 Score:  0.9950248756218906\n",
      "\n",
      "Accuracy:  0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix_F1_Score_and_Accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    This Function computes Confusion Matrix, F1 Score and Accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculating TN , FN, FP, TP\n",
    "    TN = np.sum(np.logical_and(y_true == 0, y_pred == 0))\n",
    "    FN = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n",
    "    FP = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n",
    "    TP = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n",
    "    \n",
    "    \n",
    "    # putting the values into a numpy array\n",
    "    d = np.array([[TN, FN], [FP, TP]])\n",
    "    \n",
    "    # calculating precision\n",
    "    precision = TP / (FP + TP)\n",
    "    \n",
    "    # calculating recall\n",
    "    recall = TP/ (FN + TP)\n",
    "    \n",
    "    # calculating F1 Score\n",
    "    F1_Score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Calculating accuracy\n",
    "    Accuracy = (TN + TP) / (TN + FN + FP + TP)\n",
    "    \n",
    "    # printing the results\n",
    "    print('Confusion Matrix')\n",
    "    print(pd.DataFrame(d, columns = ['Actual:0', 'Actual:1'], index = ['Predicted:0', 'Predicted:1']))\n",
    "    print('\\nF1 Score: ', F1_Score)\n",
    "    print('\\nAccuracy: ', Accuracy)\n",
    "    \n",
    "\n",
    "confusion_matrix_F1_Score_and_Accuracy(y_obs, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "##### Auc Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.48829900000000004\n"
     ]
    }
   ],
   "source": [
    "def auc(y_true, y_score):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function calculates AUC\n",
    "    \"\"\"\n",
    "    # list for storing values\n",
    "    y_pred_lst = []\n",
    "    TN_lst = []\n",
    "    FN_lst = []\n",
    "    FP_lst = []\n",
    "    TP_lst = []\n",
    "    \n",
    "    # threshold values\n",
    "    Thresholds = sorted(set(y_score), reverse = True)\n",
    "    \n",
    "    # calculating n y_pred lists according to thresold values\n",
    "    for threshold in Thresholds:\n",
    "        y_pred_lst_threshold = [1 if s >= threshold else 0 for s in y_score]\n",
    "        y_pred_lst_threshold = pd.Series(y_pred_lst_threshold)\n",
    "        y_pred_lst.append(y_pred_lst_threshold)\n",
    "    \n",
    "    # calculating TN, FN, FP, TP values\n",
    "    for y_pred in y_pred_lst:\n",
    "        TN = np.sum(np.logical_and(y_obs == 0, y_pred == 0))\n",
    "        FN = np.sum(np.logical_and(y_obs == 1, y_pred == 0))\n",
    "        FP = np.sum(np.logical_and(y_obs == 0, y_pred == 1))\n",
    "        TP = np.sum(np.logical_and(y_obs == 1, y_pred == 1))\n",
    "        TN_lst.append(TN)\n",
    "        FN_lst.append(FN)\n",
    "        FP_lst.append(FP)\n",
    "        TP_lst.append(TP)\n",
    "    \n",
    "    TN_lst = np.array(TN_lst)\n",
    "    FN_lst = np.array(FN_lst)\n",
    "    FP_lst = np.array(FP_lst)\n",
    "    TP_lst = np.array(TP_lst)\n",
    "    \n",
    "    # calculating negatives and positives\n",
    "    negative = TN_lst + FP_lst\n",
    "    positive = TP_lst + FN_lst\n",
    "    \n",
    "    # calculating TPR and FPR\n",
    "    TPR_array = TP_lst / positive\n",
    "    FPR_array = FP_lst/ negative\n",
    "    \n",
    "    # calculating AUC\n",
    "    auc = np.trapz(TPR_array, FPR_array)\n",
    "    print('AUC: ', auc)\n",
    "\n",
    "    \n",
    "auc(y_obs, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "\n",
    "<pre>\n",
    "Computing performance metrics for the given data <strong>5_b.csv</strong>\n",
    "\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [],
   "source": [
    "dt_5_b = pd.read_csv('5_b.csv')\n",
    "prob = np.asarray(dt_5_b['proba'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "source": [
    "#### Deriving Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [],
   "source": [
    "y_pred = np.asarray(list(map(lambda x: 0 if x < 0.5 else 1, prob)))\n",
    "y_obs = np.asarray(dt_5_b['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "source": [
    "#### Calculating Performance Metrics\n",
    "##### Confusion Metrics, F1 Score and Accuaracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "             Actual:0  Actual:1\n",
      "Predicted:0      9761        45\n",
      "Predicted:1       239        55\n",
      "\n",
      "F1 Score:  0.2791878172588833\n",
      "\n",
      "Accuracy:  0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix_F1_Score_and_Accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    This Function computes Confusion Matrix, F1 Score and Accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculating TN , FN, FP, TP\n",
    "    TN = np.sum(np.logical_and(y_true == 0, y_pred == 0))\n",
    "    FN = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n",
    "    FP = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n",
    "    TP = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n",
    "    \n",
    "    \n",
    "    # putting the values into a numpy array\n",
    "    d = np.array([[TN, FN], [FP, TP]])\n",
    "    \n",
    "    # calculating precision\n",
    "    precision = TP / (FP + TP)\n",
    "    \n",
    "    # calculating recall\n",
    "    recall = TP/ (FN + TP)\n",
    "    \n",
    "    # calculating F1 Score\n",
    "    F1_Score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Calculating accuracy\n",
    "    Accuracy = (TN + TP) / (TN + FN + FP + TP)\n",
    "    \n",
    "    # printing the results\n",
    "    print('Confusion Matrix')\n",
    "    print(pd.DataFrame(d, columns = ['Actual:0', 'Actual:1'], index = ['Predicted:0', 'Predicted:1']))\n",
    "    print('\\nF1 Score: ', F1_Score)\n",
    "    print('\\nAccuracy: ', Accuracy)\n",
    "\n",
    "confusion_matrix_F1_Score_and_Accuracy(y_obs, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "source": [
    "##### AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9377570000000001\n"
     ]
    }
   ],
   "source": [
    "def auc(y_true, y_score):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function calculates AUC\n",
    "    \"\"\"\n",
    "    # list for storing values\n",
    "    y_pred_lst = []\n",
    "    TN_lst = []\n",
    "    FN_lst = []\n",
    "    FP_lst = []\n",
    "    TP_lst = []\n",
    "    \n",
    "    # threshold values\n",
    "    Thresholds = sorted(set(y_score), reverse = True)\n",
    "    \n",
    "    # calculating n y_pred lists according to thresold values\n",
    "    for threshold in Thresholds:\n",
    "        y_pred_lst_threshold = [1 if s >= threshold else 0 for s in y_score]\n",
    "        y_pred_lst_threshold = pd.Series(y_pred_lst_threshold)\n",
    "        y_pred_lst.append(y_pred_lst_threshold)\n",
    "    \n",
    "    # calculating TN, FN, FP, TP values\n",
    "    for y_pred in y_pred_lst:\n",
    "        TN = np.sum(np.logical_and(y_obs == 0, y_pred == 0))\n",
    "        FN = np.sum(np.logical_and(y_obs == 1, y_pred == 0))\n",
    "        FP = np.sum(np.logical_and(y_obs == 0, y_pred == 1))\n",
    "        TP = np.sum(np.logical_and(y_obs == 1, y_pred == 1))\n",
    "        TN_lst.append(TN)\n",
    "        FN_lst.append(FN)\n",
    "        FP_lst.append(FP)\n",
    "        TP_lst.append(TP)\n",
    "    \n",
    "    TN_lst = np.array(TN_lst)\n",
    "    FN_lst = np.array(FN_lst)\n",
    "    FP_lst = np.array(FP_lst)\n",
    "    TP_lst = np.array(TP_lst)\n",
    "    \n",
    "    # calculating negatives and positives\n",
    "    negative = TN_lst + FP_lst\n",
    "    positive = TP_lst + FN_lst\n",
    "    \n",
    "    # calculating TPR and FPR\n",
    "    TPR_array = TP_lst / positive\n",
    "    FPR_array = FP_lst/ negative\n",
    "    \n",
    "    # calculating AUC\n",
    "    auc = np.trapz(TPR_array, FPR_array)\n",
    "    print('AUC: ', auc)\n",
    "\n",
    "    \n",
    "auc(y_obs, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "Computing the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "\n",
    "Predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [],
   "source": [
    "dt_5_c = pd.read_csv('5_c.csv')\n",
    "prob = np.asarray(dt_5_c['prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "source": [
    "#### Deriving the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [],
   "source": [
    "y_pred = np.asarray(list(map(lambda x: 0 if x < 0.5 else 1, prob)))\n",
    "y_obs = np.asarray(dt_5_c['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "source": [
    "#### Calculating the best threshold probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold of probability:  [0.23003903]\n"
     ]
    }
   ],
   "source": [
    "def best_threshold(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function computes the best threshold of probability which gives the lowest\n",
    "    value of metric A = 500*(number of false negative) + 100*(number of false positive)\n",
    "    \"\"\"\n",
    "    \n",
    "    Thresholds = np.array(sorted(set(prob), reverse = True))\n",
    "    \n",
    "    y_pred_lst = []\n",
    "    FN_lst = []\n",
    "    FP_lst = []\n",
    "    for threshold in Thresholds:\n",
    "        y_pred_lst_threshold = [1 if s >= threshold else 0 for s in prob]\n",
    "        y_pred_lst_threshold = pd.Series(y_pred_lst_threshold)\n",
    "        y_pred_lst.append(y_pred_lst_threshold)\n",
    "        \n",
    "    for y_pred in y_pred_lst:\n",
    "        FP = np.sum(np.logical_and(y_obs == 0, y_pred == 1))\n",
    "        FN = np.sum(np.logical_and(y_obs == 1, y_pred == 0))\n",
    "        FN_lst.append(FN)\n",
    "        FP_lst.append(FP)\n",
    "        \n",
    "    FN_lst = np.array(FN_lst)\n",
    "    FP_lst = np.array(FP_lst)\n",
    "    \n",
    "    A = 500 * FN_lst + 100 * FP_lst\n",
    "    \n",
    "    idx = np.where(A == min(A))\n",
    "    \n",
    "    print('Best Threshold of probability: ', Thresholds[idx])\n",
    "\n",
    "best_threshold(y_obs, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "Computing performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "outputs": [],
   "source": [
    "dt_5_d = pd.read_csv('5_d.csv')\n",
    "y_obs = np.asarray(dt_5_d['y'])\n",
    "y_pred = np.asarray(dt_5_d['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "#### Calculating the Performance Metrics for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error:  177.16569974554707\n",
      "\n",
      "Mean Absolute Percentage Error:  0.1291202994009687\n",
      "\n",
      "R_Squared or Coefficient of determination:  0.9563582786990937\n"
     ]
    }
   ],
   "source": [
    "def performance_metrics_regression(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function provides performance metrics for regression such as\n",
    "    MSE, MAPE, Coefficient of Determination\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculating MSE\n",
    "    MSE = np.square(y_true - y_pred).mean()\n",
    "    \n",
    "    # calculating MAPE\n",
    "    abs_Error = abs(y_pred - y_true)\n",
    "    MAPE = np.sum(abs_Error) / np.sum(y_true)\n",
    "    MAPE\n",
    "    \n",
    "    # calculating R_Squared\n",
    "    y_bar = np.mean(y_true)\n",
    "    SS_total = np.sum(np.square(y_true - y_bar))\n",
    "    SS_res = np.sum(np.square(y_true - y_pred))\n",
    "    R_Squared = 1 - (SS_res / SS_total)\n",
    "    \n",
    "    print('Mean Square Error: ', MSE)\n",
    "    print('\\nMean Absolute Percentage Error: ', MAPE)\n",
    "    print('\\nR_Squared or Coefficient of determination: ', R_Squared)\n",
    "    \n",
    "performance_metrics_regression(y_obs, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
